# OLLAMA MODELFILE GUIDE (OPTIONAL)
# This file configures how your AI model behaves with Ollama

# === WHAT IS A MODELFILE? ===
# A Modelfile is a configuration file for Ollama that allows you to:
# - Customize the behavior and personality of your AI model
# - Add specific system prompts and instructions
# - Set default parameters for inference
# - Create a tailored AI experience for your specific use case
# - Optimize context window size and performance

# === BASIC DIRECTIVES ===

#    - Specifies the base model (DeepSeek 14B is a powerful model with strong reasoning abilities)
#       Use `ollama list` to see all available models on your system
FROM deepseek-r1:14b

# SYSTEM <prompt>: Sets the system prompt that guides the model's behavior
# This is the most important part to customize your assistant's personality
# Much more effective than setting BOT_CONTEXT in your .env file
SYSTEM """
You are a helpful and friendly Discord assistant with an excellent memory.
Respond concisely and relevantly to user queries.
You should provide accurate information and be respectful at all times.
If asked about code, provide well-documented examples.
Remember details about users you interact with to provide personalized assistance.
"""

# === CONTEXT WINDOW OPTIMIZATION ===
# These parameters control how much conversation history the model can process
# DeepSeek supports up to 128K tokens of context - one of the largest windows available!
# Note: Higher values require significantly more RAM and GPU memory
#   - Discord free account limited to 2000 character (≈600 tokens)
#       The code (start.js) is already provided to truncate the message if > 2000 characters


#   - Expanded context length (32K tokens ≈ 24000 words)
#       Can be increased up to 128K if your hardware supports it
PARAMETER num_ctx 32768
#   - Number of GPUs to use for inference
PARAMETER num_gpu 1
#   - CPU threads for processing (adjust based on your hardware)
# PARAMETER num_thread 8

# === GENERATION PARAMETERS ===
# These control the quality and style of the model's responses
# More here : https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values

#   - Controls randomness (0.0 = deterministic, 1.0 = creative)
PARAMETER temperature 0.7
#   - Nucleus sampling - diversity control
PARAMETER top_p 0.9
#   - Limits token selection to top K options
PARAMETER top_k 40
#   - Penalizes repetition (higher = less repetition)
PARAMETER repeat_penalty 1.1

# === OUTPUT TOKENS ===
# DeepSeek can generate up to 32K tokens in a single response
# This is excellent for long-form content like documentation or code

# === DEEPSEEK-SPECIFIC TEMPLATE ===
# This template is specifically formatted for the DeepSeek conversation format
# The special tokens control the conversation flow
# Check the model file of your model (`ollama show --modelfile your-model`)

TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1}}
{{- if eq .Role "user" }}<｜User｜>{{ .Content }}
{{- else if eq .Role "assistant" }}<｜Assistant｜>{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}
{{- end }}
{{- if and $last (ne .Role "assistant") }}<｜Assistant｜>{{- end }}
{{- end }}"""

# === STOP SEQUENCES ===
# These tell the model when to stop generating text
# Critical for proper conversation flow with the DeepSeek model

PARAMETER stop <｜begin▁of▁sentence｜>
PARAMETER stop <｜end▁of▁sentence｜>
PARAMETER stop <｜User｜>
PARAMETER stop <｜Assistant｜>

# === HARDWARE REQUIREMENTS ===
# - For 32K context: Minimum 16GB GPU VRAM recommended
# - For 128K context: 24GB+ GPU VRAM recommended (RTX 3090 or better)
# - Consider splitting across multiple GPUs for best performance
# - SSD storage highly recommended for model loading
# If you don't have the recommended hardware:
# - Reduce num_ctx to 4096 or less (PARAMETER num_ctx 4096)
# - Consider using a smaller model like deepseek-r1:7b
# - Use the BOT_CONTEXT in .env file instead of complex Modelfile setup
# - For CPU-only systems, limit num_ctx to 2048 and increase num_thread
# - Memory usage scales with context size, so adjust accordingly
# - When using minimal settings, response quality remains good but context is limited

# === PERFORMANCE NOTES ===
# - The extended context allows DeepSeek to remember entire conversations
# - Perfect for Discord servers with long-running discussions
# - Adjust num_ctx based on your hardware capabilities
# - For most desktop systems, 16K-32K is a good balance

# === BUILD COMMAND ===
<<<<<<< HEAD
# - ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>
=======
# - `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>`
>>>>>>> aa2c3b92a37a106df2d0f8dc36d28e391a35cda2
# - Change the model name in .env to use

# For complete documentation and more examples, visit:
# https://github.com/ollama/ollama/blob/main/docs/modelfile.md
