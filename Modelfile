# OLLAMA MODELFILE GUIDE (OPTIONAL)
# This file configures how your AI model behaves with Ollama

# === WHAT IS A MODELFILE? ===
# A Modelfile is a configuration file for Ollama that allows you to:
# - Customize the behavior and personality of your AI model
# - Add specific system prompts and instructions
# - Set default parameters for inference
# - Create a tailored AI experience for your specific use case
# - Optimize context window size and performance

# === BASIC DIRECTIVES ===

FROM deepseek-r1:14b  # Specifies the base model (DeepSeek 14B is a powerful model with strong reasoning abilities)
                      # Use `ollama list` to see all available models on your system

# SYSTEM <prompt>: Sets the system prompt that guides the model's behavior
# This is the most important part to customize your assistant's personality
# Much more effective than setting BOT_CONTEXT in your .env file
SYSTEM """
You are a helpful and friendly Discord assistant with an excellent memory.
Respond concisely and relevantly to user queries.
You should provide accurate information and be respectful at all times.
If asked about code, provide well-documented examples.
Remember details about users you interact with to provide personalized assistance.
"""

# === CONTEXT WINDOW OPTIMIZATION ===
# These parameters control how much conversation history the model can process
# DeepSeek supports up to 128K tokens of context - one of the largest windows available!
# Note: Higher values require significantly more RAM and GPU memory

PARAMETER num_ctx 32768         # Expanded context length (32K tokens ≈ 24000 words)
                                # Can be increased up to 128K if your hardware supports it
PARAMETER num_gpu 1             # Number of GPUs to use for inference
#olaPARAMETER num_thread 8      # CPU threads for processing (adjust based on your hardware)

# === GENERATION PARAMETERS ===
# These control the quality and style of the model's responses

PARAMETER temperature 0.7    # Controls randomness (0.0 = deterministic, 1.0 = creative)
PARAMETER top_p 0.9          # Nucleus sampling - diversity control
PARAMETER top_k 40           # Limits token selection to top K options
PARAMETER repeat_penalty 1.1 # Penalizes repetition (higher = less repetition)

# === OUTPUT TOKENS ===
# DeepSeek can generate up to 32K tokens in a single response
# This is excellent for long-form content like documentation or code

PARAMETER max_tokens 700    # Optimized for Discord's 2000 character limit (≈600 tokens)
                            # Default setting avoids truncation in most responses
                            # Can be increased if you handle message splitting in your code

# === DEEPSEEK-SPECIFIC TEMPLATE ===
# This template is specifically formatted for the DeepSeek conversation format
# The special tokens control the conversation flow

TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1}}
{{- if eq .Role "user" }}<｜User｜>{{ .Content }}
{{- else if eq .Role "assistant" }}<｜Assistant｜>{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}
{{- end }}
{{- if and $last (ne .Role "assistant") }}<｜Assistant｜>{{- end }}
{{- end }}"""

# === STOP SEQUENCES ===
# These tell the model when to stop generating text
# Critical for proper conversation flow with the DeepSeek model

PARAMETER stop <｜begin▁of▁sentence｜>
PARAMETER stop <｜end▁of▁sentence｜>
PARAMETER stop <｜User｜>
PARAMETER stop <｜Assistant｜>

# === MEMORY OPTIMIZATION ===
# These parameters help manage memory usage for longer contexts

PARAMETER f16 true           # Use 16-bit floating point (saves memory)
PARAMETER mlock false        # Don't lock model in RAM if memory is limited
PARAMETER numa false         # Disable NUMA optimization for single-socket systems

# === HARDWARE REQUIREMENTS ===
# - For 32K context: Minimum 16GB GPU VRAM recommended
# - For 128K context: 24GB+ GPU VRAM recommended (RTX 3090 or better)
# - Consider splitting across multiple GPUs for best performance
# - SSD storage highly recommended for model loading
# If you don't have the recommended hardware:
# - Reduce num_ctx to 4096 or less (PARAMETER num_ctx 4096)
# - Consider using a smaller model like deepseek-r1:7b
# - Use the BOT_CONTEXT in .env file instead of complex Modelfile setup
# - For CPU-only systems, limit num_ctx to 2048 and increase num_thread
# - Memory usage scales with context size, so adjust accordingly
# - When using minimal settings, response quality remains good but context is limited

# === PERFORMANCE NOTES ===
# - The extended context allows DeepSeek to remember entire conversations
# - Perfect for Discord servers with long-running discussions
# - Adjust num_ctx based on your hardware capabilities
# - For most desktop systems, 16K-32K is a good balance

# === BUILD COMMAND ===
# - ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>
# - Change the model name in .env to use

# For complete documentation and more examples, visit:
# https://github.com/ollama/ollama/blob/main/docs/modelfile.md